{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-lrAIlpAamB"
      },
      "source": [
        "## Scope of This Note\n",
        "\n",
        "This note demonstrates **Stacking Ensembles in two settings**:\n",
        "\n",
        "1. **Classification Stacking**\n",
        "   - Dataset: Breast Cancer\n",
        "   - Goal: Accuracy & interpretability\n",
        "   - Meta-learner: Logistic Regression\n",
        "\n",
        "2. **Regression Stacking**\n",
        "   - Dataset: California Housing\n",
        "   - Goal: RMSE reduction\n",
        "   - Meta-learner: Ridge Regression\n",
        "\n",
        "The core stacking principles are identical; only the loss functions and evaluation metrics differ.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLb5zYzNXlLb"
      },
      "source": [
        "# Theory: The \"Manager\" Architecture\n",
        "\n",
        "In **Voting**, we assigned fixed weights (e.g., \"Trust Model A twice as much\").  \n",
        "In **Stacking**, we train a model to **learn those weights dynamically**.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## The Architecture (Two Layers)\n",
        "\n",
        "### Level 0 (Base Learners):\n",
        "We train diverse models (SVM, Tree, KNN) on the data. They make predictions.\n",
        "* **Analogy:** **The Engineers.** One is good at math, one is good at creative design.\n",
        "\n",
        "### Level 1 (Meta-Learner / Blender):\n",
        "This is a new model (usually a simple Linear Regression or Logistic Regression). It takes the predictions from Level 0 as its **inputs**.\n",
        "* **Analogy:** **The Manager.** She sees that for \"Project Type X\", the Math Engineer is usually right, so she listens to him. For \"Project Type Y\", she listens to the Creative Engineer.\n",
        "\n",
        "---\n",
        "\n",
        "## How it learns (The Trick)\n",
        "To avoid \"cheating\" (**overfitting**), we cannot train the Manager on the same data the Workers saw.\n",
        "\n",
        "1. We split the training data.\n",
        "2. The Workers train on **Part A**.\n",
        "3. They predict on **Part B**.\n",
        "4. The Manager trains on the **Predictions** made on Part B.\n",
        "\n",
        "> **Note:** Scikit-Learn handles this automatically using internal **Cross-Validation**.\n",
        "\n",
        "---\n",
        "\n",
        "## Applying the Model: The Setup\n",
        "\n",
        "We will use the **Breast Cancer Dataset**. This is ideal for Stacking because the decision boundary is complex, and different models (Linear vs. Non-linear) pick up different signal types. Stacking helps combine them to squeeze out that final 1-2% of accuracy.\n",
        "\n",
        "### Does it need scaling?\n",
        "**Yes.**\n",
        "\n",
        "* **For Base Models:** If our base models include SVM or KNN, the input data must be scaled.\n",
        "* **For Meta Model:** The meta-model sees \"predictions\" (probabilities) as input, which are already roughly scaled (0 to 1), so it's less critical there, but the input pipeline must include scaling for the workers.\n",
        "\n",
        "---\n",
        "\n",
        "## The Stacking Notebook\n",
        "We will build a pipeline where:\n",
        "\n",
        "* **Level 0:** Random Forest (Robust), SVM (Distance-based), KNN (Local patterns).\n",
        "* **Level 1:** Logistic Regression (The Manager)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPiyneCuz8pq",
        "outputId": "5c4b327e-6e36-4a4a-d906-85651a083b7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Loaded: (569, 30)\n",
            "Running Grid Search on the Stack...\n",
            "Best Params: {'stack__final_estimator__C': 1.0, 'stack__rf__n_estimators': 10}\n",
            "Best CV Score: 0.9692\n",
            "\n",
            "--- Final Results ---\n",
            "Accuracy: 0.9561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94        43\n",
            "           1       0.97      0.96      0.96        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.95      0.96      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "--- The Manager's Logic ---\n",
            "Coefficients assigned to each Base Model's predictions:\n",
            "Coefficients: [[1.93455919 3.9602202  2.43502367]]\n",
            "Order: ['RandomForest', 'SVC', 'KNN']\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. IMPORTS\n",
        "# ==========================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Level 0 Models (The Workers)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Level 1 Model (The Manager)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# The Stacking Ensemble\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION\n",
        "# ==========================================\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Data Loaded: {X.shape}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEFINE BASE LEARNERS (LEVEL 0)\n",
        "# ==========================================\n",
        "# We want diverse models. If they all make the same errors, the Manager learns nothing.\n",
        "\n",
        "# Note: We must ensure SVM outputs probabilities if we want the Manager\n",
        "# to see confidence scores rather than just hard labels.\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
        "    ('svr', SVC(probability=True, random_state=42)), # probability=True is key\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# ==========================================\n",
        "# 4. DEFINE STACKING CLASSIFIER\n",
        "# ==========================================\n",
        "# final_estimator is our Manager.\n",
        "# LogisticRegression is best here to keep the \"Management\" logic simple and interpretable.\n",
        "\n",
        "clf_stack = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(),\n",
        "    cv=5,            # Internal CV to generate training data for the Manager\n",
        "    stack_method='predict_proba', # Manager sees probabilities (0.9, 0.1) instead of labels (1, 0)\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 5. PIPELINE & TUNING\n",
        "# ==========================================\n",
        "# We wrap everything in a pipeline to handle scaling for the SVC/KNN inside the Stack.\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('stack', clf_stack)\n",
        "])\n",
        "\n",
        "\n",
        "# ‚ö†Ô∏è Note on Scaling in Stacking Pipelines:\n",
        "# Although Random Forest does not require scaling, we apply scaling globally because:\n",
        "# - SVC and KNN require scaled features\n",
        "# - Tree-based models are scale-invariant, so scaling does not harm them\n",
        "# This is a pragmatic engineering trade-off.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# HYPERPARAMETER TUNING\n",
        "# We can tune the Base Models AND the Manager simultaneously.\n",
        "# Syntax: stack__<estimator_name>__<parameter>\n",
        "# Syntax for final estimator: stack__final_estimator__<parameter>\n",
        "\n",
        "params = {\n",
        "    # Tuning a Base Model (Random Forest)\n",
        "    'stack__rf__n_estimators': [10, 50],\n",
        "\n",
        "    # Tuning the Manager (Logistic Regression)\n",
        "    # C controls how much the Manager trusts the inputs vs regularizing\n",
        "    'stack__final_estimator__C': [0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "print(\"Running Grid Search on the Stack...\")\n",
        "grid = GridSearchCV(pipeline, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Params: {grid.best_params_}\")\n",
        "print(f\"Best CV Score: {grid.best_score_:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. EVALUATION\n",
        "# ==========================================\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Final Results ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# ==========================================\n",
        "# 7. HOW IT LEARNS (INTERPRETATION)\n",
        "# ==========================================\n",
        "# Let's look at the \"Manager's\" learned weights.\n",
        "# This tells us which Base Model the Manager trusts the most.\n",
        "\n",
        "final_layer = best_model.named_steps['stack'].final_estimator_\n",
        "\n",
        "print(\"\\n--- The Manager's Logic ---\")\n",
        "print(\"Coefficients assigned to each Base Model's predictions:\")\n",
        "# The shape of coef_ depends on the classes. For binary, it's usually (1, n_estimators * n_classes)\n",
        "# or (1, n_estimators) depending on setup.\n",
        "print(f\"Coefficients: {final_layer.coef_}\")\n",
        "\n",
        "# Usually, higher coefficient = The Manager trusts this model more.\n",
        "names = ['RandomForest', 'SVC', 'KNN']\n",
        "print(f\"Order: {names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFhfDlsaX1XI"
      },
      "source": [
        "## Main Parameters Explained\n",
        "\n",
        "* **`estimators`**: A list of `(name, model)` tuples. These are the **workers**.\n",
        "* **`final_estimator`**: The model that aggregates the workers' outputs.\n",
        "    > **Recommendation:** Always start with `LogisticRegression` (for classification) or `LinearRegression` (for regression). If we use a complex model here (like a Decision Tree), the \"Manager\" might overfit to the noise in the predictions.\n",
        "* **`cv` (Cross-Validation Strategy)**:\n",
        "    This is critical. To train the Manager, the stack splits the training data (e.g., 5 folds). It trains the workers on 4 folds and predicts on the 5th. This creates \"clean\" predictions for the Manager to learn from. The meta-learner is trained only on **out-of-fold predictions**, never on predictions from models that saw the same samples during training. This prevents target leakage.\n",
        "\n",
        "    * If `cv=None`, it defaults to 5.\n",
        "* **`stack_method`**:\n",
        "    * `'auto'`: Tries to use `predict_proba` (probabilities), falls back to `decision_function`, then `predict`.\n",
        "    * **Advice:** For classification, explicit probabilities (`predict_proba`) usually give the Manager more information to work with than hard labels.\n",
        "\n",
        "---\n",
        "\n",
        "## Considerations & Trade-offs\n",
        "\n",
        "### Bias-Variance Tradeoff\n",
        "Stacking primarily reduces **bias** by combining models with different inductive assumptions.\n",
        "It can also stabilize variance, but this depends on:\n",
        "- Diversity of base learners\n",
        "- Strength and regularization of the meta-learner\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### When does this fail?\n",
        "* **Redundant Base Models:** If we stack a Random Forest and a Bagged Decision Tree, they are doing the same thing. The Manager won't find unique signals.\n",
        "* **Overfitting the Meta-Learner:** If the dataset is small, the Manager might just memorize \"When Model A is wrong, Model B is right.\"\n",
        "* **Latency:** Stacking is slow. To predict one new sample, we have to run it through all base models, then the meta-model. Not ideal for real-time, low-latency apps.\n",
        "\n",
        "---\n",
        "\n",
        "### When to use?\n",
        "* **Competitions (Kaggle):** Almost all winning solutions are stacked ensembles.\n",
        "* **Plateaued Performance:** When we have tuned our single models extensively and can't get past a performance wall (e.g., 95% accuracy). Stacking is the \"sledgehammer\" used to break that wall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "vWQ42nheaqGE",
        "outputId": "8bee836f-8b03-430b-a751-aec3d3ccea17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Shape: (16512, 8)\n",
            "Test Shape:  (4128, 8)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2ecfc058-3978-417b-aa25-0f62310f56e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>MedHouseVal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "      <td>4.526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "      <td>3.585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "      <td>3.521</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ecfc058-3978-417b-aa25-0f62310f56e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2ecfc058-3978-417b-aa25-0f62310f56e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2ecfc058-3978-417b-aa25-0f62310f56e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
              "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
              "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
              "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
              "\n",
              "   Longitude  MedHouseVal  \n",
              "0    -122.23        4.526  \n",
              "1    -122.22        3.585  \n",
              "2    -122.24        3.521  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. IMPORTS & CONFIG\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-Learn\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Regressors\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Global Config\n",
        "SEED = 42\n",
        "NP_SEED = np.random.seed(SEED)\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOAD DATA\n",
        "# ==========================================\n",
        "raw_data = fetch_california_housing(as_frame=True)\n",
        "df = raw_data.frame\n",
        "\n",
        "# In a real Kaggle comp, we would have train.csv and test.csv\n",
        "# Here, we simulate that split manually.\n",
        "X = df.drop(columns=['MedHouseVal'])\n",
        "y = df['MedHouseVal']\n",
        "\n",
        "# 80% Train (Our Playground), 20% Test (The \"Private Leaderboard\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "print(f\"Train Shape: {X_train.shape}\")\n",
        "print(f\"Test Shape:  {X_test.shape}\")\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_djJgkla0f0",
        "outputId": "c0688b84-242a-4faa-c485-0054c0bc97d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Stacking Regressor... (This might take a minute)\n",
            "\n",
            "‚úÖ Final Stacking RMSE: 0.5289\n",
            "üìä Single Random Forest RMSE: 0.5445\n",
            "üöÄ Improvement: 0.0156\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. DEFINE PREPROCESSING\n",
        "# ==========================================\n",
        "# We use RobustScaler because housing data often has outliers (mansions vs shacks)\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Note: We won't apply this globally yet. We will attach it to specific models.\n",
        "\n",
        "# ==========================================\n",
        "# 4. BASE MODELS\n",
        "# ==========================================\n",
        "\n",
        "# 1. Ridge Regression (Linear Model)\n",
        "# Good for capturing general trends. Needs Scaling.\n",
        "ridge = make_pipeline(RobustScaler(), Ridge(alpha=1.0, random_state=SEED))\n",
        "\n",
        "# 2. SVR (Support Vector Regressor)\n",
        "# Good for complex non-linear boundaries. Needs Scaling.\n",
        "# Note: SVR is slow on large data, but effective.\n",
        "svr = make_pipeline(RobustScaler(), SVR(C=1.0, epsilon=0.2))\n",
        "\n",
        "# 3. Random Forest (Bagging)\n",
        "# Good for high variance reduction. No scaling needed.\n",
        "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=SEED, n_jobs=-1)\n",
        "\n",
        "# 4. Gradient Boosting (Boosting)\n",
        "# Good for bias reduction. No scaling needed.\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=SEED)\n",
        "\n",
        "# List of tuples for the Stacker\n",
        "estimators = [\n",
        "    ('ridge', ridge),\n",
        "    ('svr', svr),\n",
        "    ('rf', rf),\n",
        "    ('gbr', gbr)\n",
        "]\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. STACKING CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# The Manager\n",
        "meta_learner = Ridge(alpha=1.0, random_state=SEED)\n",
        "\n",
        "# The Ensemble\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=estimators,\n",
        "    final_estimator=meta_learner,\n",
        "    cv=5,       # 5-fold cross-validation for training the meta-learner\n",
        "    n_jobs=-1,  # Parallelize base model training\n",
        "    passthrough=False # If True, feeds original features + predictions to meta-learner. Usually False is safer.\n",
        ")\n",
        "\n",
        "\n",
        "# Why passthrough=False?\n",
        "# Feeding original features to the meta-learner can:\n",
        "# - Increase dimensionality\n",
        "# - Reintroduce multicollinearity\n",
        "# - Cause the meta-learner to ignore base learners entirely\n",
        "\n",
        "# For small-to-medium datasets, prediction-only stacking is safer.\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 6. TRAINING & EVALUATION\n",
        "# ==========================================\n",
        "print(\"Training Stacking Regressor... (This might take a minute)\")\n",
        "stacking_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the \"Private Leaderboard\" (Test Set)\n",
        "y_pred = stacking_reg.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"\\n‚úÖ Final Stacking RMSE: {rmse:.4f}\")\n",
        "\n",
        "# Compare with a single best model (e.g., Random Forest) just to see the lift\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n",
        "print(f\"üìä Single Random Forest RMSE: {rf_rmse:.4f}\")\n",
        "print(f\"üöÄ Improvement: {rf_rmse - rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWZUQGIjbBuj",
        "outputId": "b85a31d2-cd04-4307-b76a-876dc801af4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- The Manager's Weights ---\n",
            "Intercept: -0.06\n",
            "Model [ridge]: 0.01\n",
            "Model [svr]: -0.15\n",
            "Model [rf]: 0.49\n",
            "Model [gbr]: 0.67\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 7. MODEL INTERPRETATION\n",
        "# ==========================================\n",
        "# Access the meta-learner (Ridge) from the stack\n",
        "manager = stacking_reg.final_estimator_\n",
        "\n",
        "print(\"\\n--- The Manager's Weights ---\")\n",
        "print(f\"Intercept: {manager.intercept_:.2f}\")\n",
        "for name, weight in zip([n[0] for n in estimators], manager.coef_):\n",
        "    print(f\"Model [{name}]: {weight:.2f}\")\n",
        "\n",
        "# Interpretation:\n",
        "# If 'rf' has a high weight (e.g., 0.6) and 'ridge' has low (0.1),\n",
        "# the manager relies heavily on the Forest but uses Ridge to correct slight offsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUgli8ZJb-lh"
      },
      "source": [
        "The coefficients we see are the weights assigned by the 'manager' model (our Ridge Regression final_estimator) to the predictions of each base model. In essence, they tell us how much the manager model 'trusts' or relies on each base learner when making its final prediction.\n",
        "\n",
        "Here's what our output Coefficients: [0.01, -0.15, 0.49, 0.67] for the order ['ridge', 'svr', 'rf', 'gbr'] means:\n",
        "\n",
        "ridge (0.01): The base Ridge Regression model has a very low positive weight. This suggests the manager model gives very little direct importance to the raw predictions from the first-layer Ridge model.\n",
        "\n",
        "\n",
        "svr (-0.15): The SVR base model has a negative weight. This is interesting and can indicate that the meta-learner is trying to correct for a systematic over- or under-prediction by the SVR, or perhaps the SVR's predictions are inversely related to the true value in a way the manager is exploiting. It's less common to see negative weights unless there's a specific reason for the meta-learner to 'subtract' from that model's output.\n",
        "\n",
        "\n",
        "\n",
        "rf (0.49): The Random Forest model has a moderate positive weight. This shows that the manager considers the Random Forest's predictions to be a significant and positive contributor to the final output.\n",
        "\n",
        "\n",
        "\n",
        "gbr (0.67): The Gradient Boosting Regressor has the highest positive weight. This means the manager model relies most heavily on the predictions made by the Gradient Boosting model, indicating it's considered the most influential or accurate base learner in this ensemble.\n",
        "\n",
        "\n",
        "\n",
        "In summary, the manager model learns how to optimally combine the predictions from its base models. In this case, the Gradient Boosting model's predictions are given the most weight, followed by Random Forest, while the SVR's contribution is negative, and the base Ridge model's contribution is minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHi6vXmWbZN1"
      },
      "source": [
        "Why use Stacking?\n",
        "\n",
        "It combines the strengths of different \"inductive biases.\" Trees capture non-linear steps; Linear models capture trends; SVMs capture geometric boundaries.\n",
        "\n",
        "Why CV in Stacking?\n",
        "\n",
        "If we trained base models on $X_{train}$ and then trained the meta-learner on their predictions for $X_{train}$, the meta-learner would see \"perfect\" predictions (overfitting). We use CV to ensure the meta-learner sees \"out-of-sample\" predictions (realistic errors).Bias-Variance: Stacking is primarily a Bias reduction technique (improving the fit), though it stabilizes variance too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1OJo_JiBf0w"
      },
      "source": [
        "#1: When NOT to use stacking (Data size rule)\n",
        "\n",
        "### Data Size Rule of Thumb\n",
        "Stacking works best when:\n",
        "- Dataset size is moderate to large\n",
        "- Base learners have uncorrelated errors\n",
        "\n",
        "For very small datasets, stacking often overfits due to limited out-of-fold samples.\n",
        "\n",
        "\n",
        "#2: Competition tip (this is HUGE)\n",
        "\n",
        "### Competition Tip (Kaggle)\n",
        "In high-ranking solutions:\n",
        "- First layer: many diverse models\n",
        "- Second layer: simple linear model\n",
        "- Third layer (optional): blending multiple stacks\n",
        "\n",
        "The power comes from **error diversity**, not model complexity.\n",
        "\n",
        "\n",
        "#3: One-liner summary\n",
        "\n",
        "### One-Sentence Summary\n",
        "Stacking is a supervised way of learning how much to trust each model, using cross-validated predictions to avoid overfitting.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
