{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdO-bbwTh0bv"
      },
      "source": [
        "# Theory: The \"Wisdom of Crowds\"\n",
        "\n",
        "**Imagine we are a panel of doctors diagnosing a patient.**\n",
        "\n",
        "* **Doctor A:** Expert in X-rays but misses details in blood work.\n",
        "* **Doctor B:** Expert in blood work but isn't great at X-rays.\n",
        "* **Doctor C:** A generalist who is decent at both but not an expert.\n",
        "\n",
        "If we rely on just one doctor, we might misdiagnose. But if we take a vote among all three, the experts cover each other's blind spots.\n",
        "\n",
        "---\n",
        "\n",
        "### In Machine Learning: Ensemble Learning\n",
        "A **Voting Model** is a meta-model that combines the predictions of several base models to improve generalizability and robustness compared to a single model.\n",
        "\n",
        "## Mechanism: How it Votes\n",
        "\n",
        "There are two main strategies for voting:\n",
        "\n",
        "### A. Hard Voting (Majority Rule)\n",
        "We count the specific class predictions from each classifier. The class with the most votes wins.\n",
        "\n",
        "> **Analogy:** \"3 doctors say 'Sick', 1 says 'Healthy'. The diagnosis is 'Sick'.\"\n",
        "\n",
        "* **Best for:** Classifiers that output distinct labels (or when we don't trust the probabilities).\n",
        "\n",
        "### B. Soft Voting (Weighted Average Probabilities)\n",
        "We sum the predicted probabilities (confidence scores) for each class from every classifier and average them. The class with the highest average probability wins.\n",
        "\n",
        "> **Analogy:** Doctor A says \"90% Sick\", Doctor B says \"60% Sick\", Doctor C says \"40% Healthy\" (which is 60% Sick). The average confidence is high for \"Sick\".\n",
        "\n",
        "* **Best for:** Calibrated classifiers (models that output reliable probabilities like Logistic Regression). This usually performs better than hard voting because it gives more weight to highly confident votes.\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\arg\\max_i \\sum_{j=1}^{m} w_j P_j(y=i | \\mathbf{x})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "* $w_j$ is the weight of model $j$\n",
        "* $P_j$ is the probability predicted by model $j$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZC_hPEplnSu"
      },
      "source": [
        "# Applying the Model: Basic Implementation\n",
        "\n",
        "Let's use a dataset where simple linear models might struggle, but an ensemble can succeed. We will use the Scikit-Learn Breast Cancer dataset. It is a binary classification problem ideal for checking model robustness.\n",
        "\n",
        "We will combine three different \"experts\":\n",
        "\n",
        "1. Logistic Regression (Linear expert)\n",
        "\n",
        "2. Decision Tree (Non-linear expert)\n",
        "\n",
        "3. Support Vector Machine (Distance-based expert)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZkHsjReIL1t",
        "outputId": "0185b54f-ef6f-452c-93a1-08bbace8d675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Voting Classifier Accuracy: 0.9649\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Define our \"Doctors\" (Base Estimators)\n",
        "# Note: probability=True is required for SVC if we want to use Soft Voting later\n",
        "clf1 = LogisticRegression(solver='liblinear', random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "# 3. Define the Voting Model (Hard Voting)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)],\n",
        "    voting='hard'\n",
        ")\n",
        "\n",
        "# 4. Train\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "print(f\"Voting Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD7-md-NmErZ"
      },
      "source": [
        "### Main Parameters Explained\n",
        "\n",
        "* **`estimators`**:\n",
        "    A list of tuples, e.g., `[('name1', model1), ('name2', model2)]`. This stores the diverse models we want to vote on.\n",
        "\n",
        "* **`voting`**:\n",
        "    * `'hard'`: Majority rule (default).\n",
        "    * `'soft'`: Average probabilities.\n",
        "        > **Crucial:** All base models must support `predict_proba()` for this to work.\n",
        "\n",
        "* **`weights`**:\n",
        "    *(Optional)* A list of floats, e.g., `[1, 2, 1]`. Use this if we trust one model (e.g., the Decision Tree) twice as much as the others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfvoQpG2mSHW"
      },
      "source": [
        "## Advanced Pipeline & Hyperparameter Tuning\n",
        "Now, let's build a production-grade pipeline.\n",
        "\n",
        "### Why Scaling?\n",
        "**Yes, data scaling is critical here.** * **The Reason:** Our ensemble includes **Logistic Regression** and **SVM**. These models calculate distances or weights based on feature magnitude.\n",
        "* **The Example:** If one feature is \"Income ($100,000$)\" and another is \"Age ($50$)\", the model will be biased toward Income.\n",
        "* **The Exception:** A **Decision Tree** doesn't care about scale, but the **Voting Classifier** (containing SVM/LogReg) will fail without it.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Implementation Details\n",
        "We will use a different dataset: **The Wine Dataset** (Multiclass classification). We will tune the hyperparameters of the internal models using `GridSearchCV`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85frhdYImGjB",
        "outputId": "ef6b229e-9ad2-4883-c8e0-0608a4cd02ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Params: {'voter__dt__max_depth': 3, 'voter__lr__C': 1.0, 'voter__weights': [1, 1, 1]}\n",
            "Best CV Score: 0.9833\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# 1. Load Data\n",
        "wine = load_wine()\n",
        "X_wine, y_wine = wine.data, wine.target\n",
        "\n",
        "# 2. Build Pipeline\n",
        "# We need scaling first, then the voting classifier\n",
        "clf1 = LogisticRegression(solver='liblinear', random_state=42)\n",
        "clf2 = DecisionTreeClassifier(random_state=42)\n",
        "clf3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('voter', voting_clf)\n",
        "])\n",
        "\n",
        "# 3. Hyperparameter Tuning\n",
        "# NOTE: To tune parameters of models INSIDE the voter, we use double underscores.\n",
        "# Format: <step_name>__<estimator_name>__<parameter>\n",
        "param_grid = {\n",
        "    'voter__lr__C': [0.1, 1.0, 10],       # Tuning C for Logistic Regression\n",
        "    'voter__dt__max_depth': [3, 5, None], # Tuning depth for Decision Tree\n",
        "    'voter__weights': [[1, 1, 1], [2, 1, 1], [1, 2, 1]] # Tuning influence of models\n",
        "}\n",
        "\n",
        "# Grid Search with Cross Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_wine, y_wine)\n",
        "\n",
        "print(f\"Best Params: {grid_search.best_params_}\")\n",
        "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETy_muytmomb"
      },
      "source": [
        "## Critical Considerations\n",
        "\n",
        "### 1. Bias-Variance Tradeoff\n",
        "* **Variance Reduction:** This is the primary superpower of Voting Models. If a Decision Tree has high variance (overfits) and a Logistic Regression has high bias (underfits), averaging them tends to smooth out the noise. The ensemble usually has **lower variance** than single models.\n",
        "* **Bias:** The ensemble bias is usually similar to the average bias of the base models. It won't magically fix a problem if all our models are underfitting (high bias).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 2. When does this fail?\n",
        "* **Correlated Errors:** If all our models make the same mistakes (e.g., they all struggle with the same specific edge case), voting changes nothing. Three wrong votes is still a wrong decision.\n",
        "* **Overwhelmingly Bad Model:** If we have two terrible models and one great model, the two bad ones will outvote the expert. (*Note: Always check individual performance before ensemble inclusion.*)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. When to use?\n",
        "* **Competition/High Stakes:** When every 0.1% accuracy counts (Kaggle competitions often use this).\n",
        "* **Ambiguous Data:** When the decision boundary is fuzzy and different models \"see\" the data differently.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Metric & Interpretability\n",
        "* **Metric:** We need a metric like **Accuracy** or **F1-Score**.\n",
        "* **Tradeoff:** Voting Classifiers are harder to interpret (\"Black Box\") than a single Decision Tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uqst5CNAmqbV",
        "outputId": "40cf2358-7475-42e7-cb1d-1f4b7c43a579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape: (569, 30)\n",
            "Training Samples: 455, Test Samples: 114\n",
            "\n",
            "Starting Hyperparameter Tuning (this may take a moment)...\n",
            "\n",
            "Best Parameters Found:\n",
            "{'voting__knn__n_neighbors': 5, 'voting__lr__C': 10, 'voting__weights': [1, 1, 1]}\n",
            "Best CV Accuracy: 0.9802\n",
            "\n",
            "--- Final Test Set Results ---\n",
            "Accuracy: 0.9649\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.95      0.95      0.95        43\n",
            "      benign       0.97      0.97      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.96      0.96       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "\n",
            "--- Anatomy of a Vote (Sample 0) ---\n",
            "Model [lr] says: Benign: 0.89, Malignant: 0.11\n",
            "Model [knn] says: Benign: 1.00, Malignant: 0.00\n",
            "Model [svc] says: Benign: 0.98, Malignant: 0.02\n",
            "--> FINAL Weighted Vote: Benign: 0.96, Malignant: 0.04\n",
            "True Label: Benign\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. IMPORTS & SETUP\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Base Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Ensemble\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION\n",
        "# ==========================================\n",
        "# We use Breast Cancer dataset: 30 features, binary target (Malignant/Benign)\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data (Essential to avoid data leakage during testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Dataset Shape: {X.shape}\")\n",
        "print(f\"Training Samples: {X_train.shape[0]}, Test Samples: {X_test.shape[0]}\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEFINING BASE LEARNERS\n",
        "# ==========================================\n",
        "# We choose diverse models to maximize ensemble benefit.\n",
        "# 1. Logistic Regression (Linear)\n",
        "# 2. KNN (Distance based - requires scaling)\n",
        "# 3. SVC (Complex boundaries - requires scaling)\n",
        "\n",
        "# Note: We do NOT scale X_train here manually.\n",
        "# We will put scaling inside the Pipeline to prevent leakage during CV.\n",
        "\n",
        "clf_log = LogisticRegression(solver='liblinear', random_state=42)\n",
        "clf_knn = KNeighborsClassifier()\n",
        "clf_svc = SVC(probability=True, random_state=42) # probability=True needed for soft voting\n",
        "\n",
        "# ==========================================\n",
        "# 4. BUILDING THE VOTING PIPELINE\n",
        "# ==========================================\n",
        "# We create the voting classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('lr', clf_log),\n",
        "        ('knn', clf_knn),\n",
        "        ('svc', clf_svc)\n",
        "    ],\n",
        "    voting='soft' # Soft voting generally outperforms hard voting on this dataset\n",
        ")\n",
        "\n",
        "# We wrap it in a pipeline to handle scaling automatically\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),  # Scale data first!\n",
        "    ('voting', voting_clf)         # Then vote\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# 5. HYPERPARAMETER TUNING (GridSearch)\n",
        "# ==========================================\n",
        "# We want to tune the parameters of the base models *through* the voting classifier.\n",
        "# Syntax: voting__<estimator_name>__<param>\n",
        "\n",
        "params = {\n",
        "    'voting__lr__C': [0.1, 1.0, 10],      # Regularization for LogReg\n",
        "    'voting__knn__n_neighbors': [3, 5, 7], # Neighbors for KNN\n",
        "    'voting__weights': [[1, 1, 1], [2, 1, 1], [1, 2, 1]] # Weighting strategy\n",
        "}\n",
        "\n",
        "print(\"\\nStarting Hyperparameter Tuning (this may take a moment)...\")\n",
        "grid = GridSearchCV(pipeline, params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest Parameters Found:\\n{grid.best_params_}\")\n",
        "print(f\"Best CV Accuracy: {grid.best_score_:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. FINAL EVALUATION\n",
        "# ==========================================\n",
        "# Predict on the held-out test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\n--- Final Test Set Results ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "# ==========================================\n",
        "# 7. HOW IT LEARNS (INTERPRETATION)\n",
        "# ==========================================\n",
        "# To see how the voting actually happened for a specific sample:\n",
        "sample_id = 0\n",
        "sample_data = X_test[sample_id].reshape(1, -1)\n",
        "true_label = y_test[sample_id]\n",
        "\n",
        "# Access the voter inside the pipeline\n",
        "voter = best_model.named_steps['voting']\n",
        "scaler = best_model.named_steps['scaler']\n",
        "scaled_sample = scaler.transform(sample_data)\n",
        "\n",
        "# Get probabilities from each internal estimator\n",
        "print(f\"\\n--- Anatomy of a Vote (Sample {sample_id}) ---\")\n",
        "for name, method in voter.named_estimators_.items():\n",
        "    prob = method.predict_proba(scaled_sample)[0]\n",
        "    print(f\"Model [{name}] says: Benign: {prob[1]:.2f}, Malignant: {prob[0]:.2f}\")\n",
        "\n",
        "final_prob = best_model.predict_proba(sample_data)[0]\n",
        "print(f\"--> FINAL Weighted Vote: Benign: {final_prob[1]:.2f}, Malignant: {final_prob[0]:.2f}\")\n",
        "print(f\"True Label: {'Benign' if true_label==1 else 'Malignant'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-gZoA-qoBZj"
      },
      "source": [
        "# Theory: Averaging \"Opinions\"\n",
        "\n",
        "In classification, we voted on a label. In regression, we cannot \"vote\" because the outputs are continuous numbers (e.g., House Price). Instead, we take the **Average**.\n",
        "\n",
        "**Imagine we are appraising a house:**\n",
        "* **Agent A (Linear Model):** says $500k\n",
        "* **Agent B (Decision Tree):** says $550k\n",
        "* **Agent C (SVR):** says $480k\n",
        "\n",
        "The Voting Regressor calculates the average prediction:\n",
        "$$\\frac{500 + 550 + 480}{3} = \\$510k$$\n",
        "\n",
        "By averaging these diverse predictions, we smooth out the error. If Agent B overestimates and Agent C underestimates, they cancel each other out, leaving us closer to the truth.\n",
        "\n",
        "### The Mathematical Formula\n",
        "$$\\hat{y} = \\frac{1}{\\sum w_j} \\sum_{j=1}^{m} w_j f_j(\\mathbf{x})$$\n",
        "\n",
        "**Where:**\n",
        "* $w_j$ is the weight of the $j$-th model.\n",
        "* $f_j(\\mathbf{x})$ is the prediction of that model.\n",
        "\n",
        "---\n",
        "\n",
        "# Applying the Model: The Regression Pipeline\n",
        "\n",
        "We will use the **California Housing Dataset**. This is a classic regression problem (predicting house values) that benefits from scaling and ensemble methods.\n",
        "\n",
        "### Our Strategy:\n",
        "* **Base Models:**\n",
        "    * `LinearRegression`: Captures global linear trends.\n",
        "    * `DecisionTreeRegressor`: Captures non-linear local patterns.\n",
        "    * `SVR` (Support Vector Regressor): Captures complex boundaries (needs scaling).\n",
        "* **Pipeline:** We must scale the data because SVR and Linear Regression are sensitive to feature magnitude.\n",
        "* **Tuning:** We will tune the weights to see which \"Agent\" we should trust the most."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqJwzh_1oHBv",
        "outputId": "841ea710-6e24-4dba-cc6e-bf96082fac50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 16512\n",
            "Feature scale example (feature 0): Mean=3.88, Std=1.90\n",
            "\n",
            "Running Grid Search (tuning ensemble weights)...\n",
            "Best Params: {'voter__svr__C': 10.0, 'voter__weights': [1, 1, 2]}\n",
            "Best RMSE (CV): 0.5870\n",
            "\n",
            "--- Final Test Set Results ---\n",
            "R2 Score: 0.7274 (1.0 is perfect)\n",
            "RMSE: 0.5977\n",
            "\n",
            "--- How the prediction was made (Sample 0) ---\n",
            "Model [lr] predicts: 0.719\n",
            "Model [dt] predicts: 1.169\n",
            "Model [svr] predicts: 0.490\n",
            "--> Ensemble Weighted Average: 0.717\n",
            "--> Actual Model Output: 0.717\n",
            "--> True Value: 0.477\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. IMPORTS & SETUP\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Base Regressors\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor # Let's add a strong one\n",
        "\n",
        "# The Ensemble\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION\n",
        "# ==========================================\n",
        "# California Housing: Predict median house value\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Feature scale example (feature 0): Mean={X_train[:,0].mean():.2f}, Std={X_train[:,0].std():.2f}\")\n",
        "# Note: Since features have different scales, scaling is mandatory for SVR.\n",
        "\n",
        "# ==========================================\n",
        "# 3. DEFINING BASE LEARNERS\n",
        "# ==========================================\n",
        "reg1 = LinearRegression()\n",
        "reg2 = DecisionTreeRegressor(max_depth=5, random_state=42) # Constrain depth to reduce overfitting\n",
        "reg3 = SVR(kernel='rbf')\n",
        "\n",
        "# ==========================================\n",
        "# 4. BUILDING THE PIPELINE\n",
        "# ==========================================\n",
        "# Define Voting Regressor\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('lr', reg1),\n",
        "        ('dt', reg2),\n",
        "        ('svr', reg3)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Pipeline: Scaler -> Voter\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('voter', voting_reg)\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# 5. HYPERPARAMETER TUNING\n",
        "# ==========================================\n",
        "# We will tune the \"weights\" (how much we trust each model)\n",
        "# and the SVR's regularization parameter 'C'.\n",
        "\n",
        "params = {\n",
        "    'voter__weights': [[1, 1, 1], [1, 2, 1], [1, 1, 2]], # Equal trust vs trusting DT/SVR more\n",
        "    'voter__svr__C': [1.0, 10.0]  # Tuning the SVR inside the voter\n",
        "}\n",
        "\n",
        "print(\"\\nRunning Grid Search (tuning ensemble weights)...\")\n",
        "grid = GridSearchCV(pipeline, params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Params: {grid.best_params_}\")\n",
        "# Note: Scoring is negative MSE in sklearn (higher is better), so we flip sign\n",
        "print(f\"Best RMSE (CV): {np.sqrt(-grid.best_score_):.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. EVALUATION\n",
        "# ==========================================\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"\\n--- Final Test Set Results ---\")\n",
        "print(f\"R2 Score: {r2:.4f} (1.0 is perfect)\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. ANATOMY OF A PREDICTION\n",
        "# ==========================================\n",
        "# Let's peek under the hood for the first test sample\n",
        "sample = X_test[0].reshape(1, -1)\n",
        "true_val = y_test[0]\n",
        "\n",
        "# Access steps\n",
        "scaler = best_model.named_steps['scaler']\n",
        "voter = best_model.named_steps['voter']\n",
        "scaled_sample = scaler.transform(sample)\n",
        "\n",
        "print(f\"\\n--- How the prediction was made (Sample 0) ---\")\n",
        "predictions = []\n",
        "for name, estimator in voter.named_estimators_.items():\n",
        "    pred = estimator.predict(scaled_sample)[0]\n",
        "    predictions.append(pred)\n",
        "    print(f\"Model [{name}] predicts: {pred:.3f}\")\n",
        "\n",
        "weights = grid.best_params_['voter__weights']\n",
        "# Calculate weighted average manually to prove concept\n",
        "manual_pred = np.average(predictions, weights=weights)\n",
        "\n",
        "print(f\"--> Ensemble Weighted Average: {manual_pred:.3f}\")\n",
        "print(f\"--> Actual Model Output: {best_model.predict(sample)[0]:.3f}\")\n",
        "print(f\"--> True Value: {true_val:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hksIiwikor73"
      },
      "source": [
        "## Bias-Variance Tradeoff\n",
        "\n",
        "* **Variance:** Like the classifier, the main goal here is **Variance Reduction**.\n",
        "* **Intuition:** If the Linear Regression is too rigid (**High Bias**) and the Decision Tree is too chaotic (**High Variance**), the SVR might find a middle ground. Averaging them dampens the outliers produced by the Decision Tree.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## When to use Voting Regressor?\n",
        "\n",
        "1.  **Uncertainty in Model Selection:** When we don't know if the data is linear or non-linear, we mix both.\n",
        "2.  **Improving Stability:** Single Decision Trees change drastically with small data changes. An ensemble of a Tree + Linear Regression is much more stable.\n",
        "3.  **High Stakes Prediction:** Used in financial forecasting where reliability is more important than interpretability.\n",
        "\n",
        "---\n",
        "\n",
        "## When does it fail?\n",
        "\n",
        "> [!CAUTION]\n",
        "> **Outliers:** If one of our models produces an extreme outlier (e.g., predicts $10M instead of $1M), it can skew the average significantly. Unlike classification, where a wrong vote is just one vote, in regression, the **magnitude** of the error matters.\n",
        "\n",
        "* **Solution:** Check the individual $R^2$ scores of base models. If one model is performing significantly worse than the others, remove it from the ensemble.\n",
        "\n",
        "---\n",
        "\n",
        "## Does it need scaling?\n",
        "\n",
        "**Yes.** We are using `StandardScaler` in the pipeline.\n",
        "\n",
        "* **The Reason:** The Voting Regressor itself is just a mathematical average, but the **Base Models** inside it (such as SVR, Linear Regression, or KNN) absolutely require scaled data to function correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1IX0ckJpR6w"
      },
      "source": [
        "# Theory: The \"Parallel Universe\" Strategy\n",
        "\n",
        "In the **Voting Model**, we used different types of \"doctors\" (Linear, SVM, Tree).  \n",
        "In **Bagging**, we use the **same type of doctor** (usually a Decision Tree), but we train them on different variations of the training data.\n",
        "\n",
        "---\n",
        "\n",
        "## Two Key Steps:\n",
        "\n",
        "### 1. Bootstrapping (The \"Bootstrap\")\n",
        "We create multiple random subsets of our original training data. Crucially, we sample **with replacement**.\n",
        "\n",
        "* **Imagine:** We have a deck of flashcards. We draw a card, write it down, put it back in the deck, and shuffle. We do this until we have a new deck.\n",
        "* **Result:** Some cards appear 2-3 times, others (about 37%) never appear. These missing cards are called **Out-of-Bag (OOB)** instances.\n",
        "\n",
        "### 2. Aggregating (The \"Aggregating\")\n",
        "We train a separate model on each \"bootstrapped\" dataset. Because the datasets are slightly different, the models will be slightly different.\n",
        "\n",
        "* **Classification:** We take the majority vote.\n",
        "* **Regression:** We take the average.\n",
        "\n",
        "---\n",
        "\n",
        "## Why do we do this?\n",
        "**To kill Variance.**\n",
        "\n",
        "\n",
        "\n",
        "A single Decision Tree is very sensitive to noise (high variance). By averaging 100 trees trained on slightly different data, the noise cancels out, and the true pattern remains.\n",
        "\n",
        "---\n",
        "\n",
        "##  Applying the Model: Single Tree vs. Bagging\n",
        "\n",
        "Let's prove the theory. We will compare a single **Decision Tree** against a **Bagging Classifier** on the Digits dataset.\n",
        "\n",
        "### Does it need scaling?\n",
        "**Usually No.** Bagging is typically used with Decision Trees, and Trees do not require scaling.\n",
        "\n",
        "* **Exception:** If we bag models that *do* need scaling (like Bagged SVMs or Bagged KNN), then yes, we must scale.\n",
        "* **Best Practice:** For this example (Trees), we can skip it, but we will include it in the pipeline just to be safe for future model swaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r36QNcqIpV3G",
        "outputId": "7ab5133e-6e86-491e-9ec1-ab4a0ee2283b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Shape: (1797, 64) (1797 samples, 64 pixels/features)\n",
            "Single Decision Tree Accuracy: 0.8417\n",
            "\n",
            "Running Grid Search (this handles the CV)...\n",
            "Best Params: {'bagging__max_features': 0.5, 'bagging__max_samples': 0.7, 'bagging__n_estimators': 200}\n",
            "Best CV Score: 0.9694\n",
            "\n",
            "OOB Score (Validation estimate): 0.9701\n",
            "Final Test Accuracy: 0.9722\n",
            "Improvement over Single Tree: +13.06%\n",
            "\n",
            "--- Insight ---\n",
            "We trained multiple trees on random subsets.\n",
            "Because we set max_features=0.5,\n",
            "each tree also only saw a portion of the pixels!\n",
            "This forces the trees to be diverse, making the vote robust.\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 1. IMPORTS\n",
        "# ==========================================\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# ==========================================\n",
        "# 2. DATA PREPARATION (Digits Dataset)\n",
        "# ==========================================\n",
        "# The Digits dataset: 8x8 pixel images of numbers 0-9.\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# Split Data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Data Shape: {X.shape} (1797 samples, 64 pixels/features)\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. ESTABLISH A BASELINE\n",
        "# ==========================================\n",
        "# Let's see how a single Decision Tree performs.\n",
        "# Trees tend to overfit (High training score, lower test score).\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "base_acc = single_tree.score(X_test, y_test)\n",
        "print(f\"Single Decision Tree Accuracy: {base_acc:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. BUILD THE BAGGING PIPELINE\n",
        "# ==========================================\n",
        "# We use a Pipeline even though Trees don't need scaling,\n",
        "\n",
        "\n",
        "# The Base Learner: A Decision Tree\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# The Bagging Ensemble\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_estimator,\n",
        "    n_estimators=100,      # 100 Trees\n",
        "    max_samples=0.8,       # Each tree sees 80% of the training data\n",
        "    oob_score=True,        # Use the left-out data for validation automatically\n",
        "    random_state=42,\n",
        "    n_jobs=-1              # Use all CPU cores (Bagging is parallel!)\n",
        ")\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()), # Optional for trees, critical for Bagged SVM/KNN\n",
        "    ('bagging', bagging_clf)\n",
        "])\n",
        "\n",
        "# ==========================================\n",
        "# 5. HYPERPARAMETER TUNING\n",
        "# ==========================================\n",
        "# We tune:\n",
        "# 1. n_estimators: How many trees?\n",
        "# 2. max_samples: How much data does each tree get?\n",
        "# 3. max_features: Can we limit features too? (This moves us closer to Random Forest)\n",
        "\n",
        "param_grid = {\n",
        "    'bagging__n_estimators': [50, 100, 200],\n",
        "    'bagging__max_samples': [0.5, 0.7, 1.0],\n",
        "    'bagging__max_features': [0.5, 1.0] # train on 50% or 100% of features\n",
        "}\n",
        "\n",
        "print(\"\\nRunning Grid Search (this handles the CV)...\")\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Params: {grid.best_params_}\")\n",
        "print(f\"Best CV Score: {grid.best_score_:.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. FINAL EVALUATION & OOB SCORE\n",
        "# ==========================================\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Retrieve OOB Score (Out-of-Bag Score)\n",
        "# This is an estimate of test accuracy calculated during training without looking at X_test\n",
        "oob_score = best_model.named_steps['bagging'].oob_score_\n",
        "print(f\"\\nOOB Score (Validation estimate): {oob_score:.4f}\")\n",
        "\n",
        "# Final Test Prediction\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Final Test Accuracy: {final_acc:.4f}\")\n",
        "print(f\"Improvement over Single Tree: +{(final_acc - base_acc)*100:.2f}%\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. HOW IT LEARNS (INTERPRETATION)\n",
        "# ==========================================\n",
        "print(\"\\n--- Insight ---\")\n",
        "print(\"We trained multiple trees on random subsets.\")\n",
        "print(f\"Because we set max_features={grid.best_params_['bagging__max_features']},\")\n",
        "print(\"each tree also only saw a portion of the pixels!\")\n",
        "print(\"This forces the trees to be diverse, making the vote robust.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzsK-uLxpl1r"
      },
      "source": [
        "# Advanced Topics & Considerations\n",
        "\n",
        "## The \"Out-of-Bag\" (OOB) Concept\n",
        "This is a unique advantage of Bagging.\n",
        "\n",
        "\n",
        "\n",
        "* **How it works:** Since we sample with replacement, about **37%** of the training data is never seen by a specific tree.\n",
        "* **The Benefit:** We can use this \"leftover\" data to test that specific tree.\n",
        "* **OOB Score:** Averaging these tests gives us the OOB Score, which acts like a **Validation Set** without us needing to split the data manually.\n",
        "\n",
        "---\n",
        "\n",
        "## Bias-Variance Tradeoff\n",
        "\n",
        "* **Variance:** Bagging is the \"King of Variance Reduction.\" It smooths out the \"jitters\" of complex models.\n",
        "* **Bias:** Bagging **cannot** fix high bias. If we bag a simple Linear Regression (which underfits), the average of 100 underfitting models is still an underfitting model.\n",
        "* **Rule of Thumb:** Bagging works best on **\"Strong, Complex\"** models (like deep Decision Trees).\n",
        "\n",
        "---\n",
        "\n",
        "## Sampling Variations\n",
        "\n",
        "### Bagging vs. Pasting\n",
        "* **Bagging:** Sampling **with** replacement (the same data point can be chosen twice for one tree). This is the default and usually reduces variance better.\n",
        "* **Pasting:** Sampling **without** replacement. Used sometimes for massive datasets.\n",
        "\n",
        "### Random Patches vs. Random Subspaces\n",
        "[Image diagram comparing Random Patches and Random Subspaces sampling]\n",
        "\n",
        "* **Random Subspaces:** Sampling features (columns) but using all samples (rows). Good when we have very high feature counts (e.g., DNA data).\n",
        "* **Random Patches:** Sampling **both** features (columns) and samples (rows).\n",
        "\n",
        "---\n",
        "\n",
        "## When does this fail?\n",
        "\n",
        "* **Computationally Expensive:** Training 100 models takes 100x the time (though it can be parallelized using `n_jobs=-1`).\n",
        "* **Loss of Interpretability:** A single Decision Tree is easy to visualize (Yes/No path). A bag of 100 trees is a **\"Black Box\"**â€”we know it works, but it's hard to explain exactly *why* a specific decision was made."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
