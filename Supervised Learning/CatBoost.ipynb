{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aI9Q5os14x1c"
      },
      "source": [
        "# In-Depth CatBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yda2Tgy48NS"
      },
      "source": [
        "CatBoost is a gradient-boosted decision tree library developed by Yandex that shines when our data contains categorical features (it natively handles them), and it includes algorithmic fixes (ordered boosting / permutation-based schemes) to reduce target-leakage and overfitting for categorical encodings. It uses symmetric (oblivious) trees, has strong default regularization, and supports CPU/GPU training, multi-class, regression, ranking, text features, and model interpretation (SHAP).\n",
        "\n",
        "### Gradient boosting backbone\n",
        "\n",
        "CatBoost is a gradient boosting framework: we sequentially add trees that fit the negative gradient (residual) of the loss function. Each new tree aims to reduce the ensemble's loss.\n",
        "\n",
        "### Symmetric (oblivious) trees\n",
        "\n",
        "Trees in CatBoost are symmetric: each tree level uses the same split condition for all nodes at that level. This makes trees balanced and fast at prediction (bitwise decisions), reduces overfitting, and often yields smaller, faster models.\n",
        "\n",
        "### Ordered boosting and categorical handling\n",
        "\n",
        "A classic problem with naive target statistics (mean target per category) is target leakage: if we compute mean target using all training labels, the model can learn to “peek” at the label.\n",
        "\n",
        "CatBoost uses ordered target statistics and permutations: for each object it only uses preceding objects in a permutation to compute category statistics — this reduces leakage. The algorithm also supports multiple permutations (ensembling within training) to stabilize estimates.\n",
        "\n",
        "CatBoost accepts categorical features directly (via cat_features), computes special statistics internally, and applies transformations that avoid leakage.\n",
        "\n",
        "### Leaf value computation\n",
        "\n",
        "Rather than the simplest gradient-descent leaf updates, CatBoost uses second-order approximations (Newton-like updates) in some cases and specialized regularization to compute leaf values robustly.\n",
        "\n",
        "### Regularization techniques\n",
        "\n",
        "L2 (via l2_leaf_reg), random strength (random_strength) which perturbs splits, bagging_temperature for Bayesian bootstrap-style sampling, border_count for feature binning, and od_type / od_wait for ordered/IncToDec early stopping — plus symmetric trees themselves act as implicit regularizers.\n",
        "\n",
        "\n",
        "### Missing values\n",
        "\n",
        "Treated as a separate bin — no imputation required.\n",
        "\n",
        "### Categorical cardinality\n",
        "\n",
        "For high cardinality features CatBoost's statistics + combinations (one can combine categoricals) are designed to work well; but extremely high-cardinality features may still need special consideration.\n",
        "\n",
        "### CatBoost Hyperparameters Reference\n",
        "\n",
        "| Hyperparameter | Description | Typical Range / Values |\n",
        "| :--- | :--- | :--- |\n",
        "| **`iterations`** | The maximum number of trees to build (**n_estimators**). | `500` – `2000` (Use with early stopping) |\n",
        "| **`learning_rate`** | The step size shrinkage used to prevent overfitting (**eta**). | `0.01` – `0.3` (Lower needs more iterations) |\n",
        "| **`depth`** | Depth of the symmetric trees. | `4` – `10` |\n",
        "| **`l2_leaf_reg`** | L2 regularization coefficient on leaf weights. | `1` – `10` |\n",
        "| **`bagging_temperature`**| Controls Bayesian bootstrap. `0` is no bagging; higher is more aggressive. | `0` – `1` (Up to `10`) |\n",
        "| **`random_strength`** | Score randomness used to diversify tree splits. | `1` – `20` |\n",
        "| **`border_count`** | Number of splits for numerical feature binning (discretization). | `32` – `255` |\n",
        "| **`od_type` / `od_wait`** | Overfitting detector type (`IncToDec`, `Iter`) and patience (iterations). | `Iter`, `20` – `50` |\n",
        "| **`task_type`** | The processing unit to be used for training. | `'CPU'` or `'GPU'` |\n",
        "\n",
        "---\n",
        "\n",
        "### Handling Categorical & Imbalanced Data\n",
        "\n",
        "* **`cat_features`**: List of indices or names for categorical columns. CatBoost handles these natively.\n",
        "* **`one_hot_max_size`**: Categories with a size $\\le$ this value use one-hot encoding; larger ones use mean encoding.\n",
        "* **`class_weights`**: Manual weights for imbalanced classes (e.g., `[1, 5]`).\n",
        "* **`auto_class_weights`**: Automatically balance classes (`'Balanced'`, `'SqrtBalanced'`).\n",
        "\n",
        "### Training & Objectives\n",
        "\n",
        "* **`loss_function`**: The metric used for training (e.g., `Logloss`, `CrossEntropy`, `RMSE`).\n",
        "* **`eval_metric`**: The metric used for validation and early stopping (e.g., `AUC`, `F1`, `MAE`).\n",
        "* **`use_best_model`**: If `True`, the model will revert to the iteration with the best `eval_metric`.\n",
        "\n",
        "\n",
        "### How the model learns — stepwise summary\n",
        "\n",
        "1. Preprocessing: numeric features are binned (border_count), categorical handled via ordered statistics or one-hot (small cardinality).\n",
        "\n",
        "2. For each iteration:\n",
        "\n",
        "   *   Compute gradients (residuals) using the loss.\n",
        "\n",
        "   *   Train a symmetric tree to approximate gradients; node splits chosen based on loss reduction calculated from binned features (and statistical transforms for categorical features).\n",
        "\n",
        "   *    Compute leaf values with regularization using gradient and (optionally) second-order info.\n",
        "\n",
        "4. Optionally use bagging (Bayesian bootstrap via bagging_temperature) for variance reduction.\n",
        "\n",
        "5.  Use ordered boosting for permutations to avoid target leakage with categorical features.\n",
        "\n",
        "\n",
        "### Is Feature Scaling Required for CatBoost?\n",
        "\n",
        "**Short Answer: No.** Unlike distance-based algorithms (like KNN or SVM) or gradient-based models (like Neural Networks), CatBoost is an ensemble of **decision trees**. Decision trees are **invariant to monotonic transformations**.\n",
        "\n",
        "### Why Scaling Isn't Necessary\n",
        "* **Split Invariance:** Trees make decisions based on thresholds (e.g., `feature > 100`). Whether that 100 is scaled to 0.5 or 1,000,000, the \"split\" point remains functionally identical.\n",
        "* **Rank-Based:** CatBoost looks at the relative order of values rather than their absolute magnitude.\n",
        "\n",
        "---\n",
        "\n",
        "### ⚠️ Exceptions: When we *Should* Scale\n",
        "While CatBoost doesn't need it, our **overall pipeline** might:\n",
        "\n",
        "1.  **Stacked Ensembles:** If we are feeding CatBoost predictions into a \"meta-learner\" (like a Logistic Regression) or combining it with distance-based models (KNN), those secondary models will still require scaled input.\n",
        "2.  **Downstream Linear Models:** If we use the same preprocessed data for a Linear Regression baseline, scaling remains mandatory for that specific model.\n",
        "3.  **Manual Encoding:** If we perform manual one-hot encoding or other transformations that we intend to use with non-tree models downstream.\n",
        "\n",
        "---\n",
        "\n",
        "> **Conclusion for Standalone CatBoost:** > we can skip `StandardScaler` or `MinMaxScaler` entirely. This saves computation time and keeps our features interpretable (e.g., seeing \"Price > $500\" in our feature importance is easier to read than \"Price > 0.12\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLwbsjtU8Dz_"
      },
      "source": [
        "# Practical demo 1 — classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRn-MtudNIvd",
        "outputId": "03d4c6e0-f658-49b6-b077-7a9412505561"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "0:\ttest: 0.9742063\tbest: 0.9742063 (0)\ttotal: 59.5ms\tremaining: 59.4s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 0.9973544974\n",
            "bestIteration = 5\n",
            "\n",
            "Shrink model to first 6 iterations.\n",
            "Accuracy: 0.9736842105263158\n",
            "AUC: 0.9973544973544973\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96        42\n",
            "           1       0.97      0.99      0.98        72\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Demo 1: Quick baseline with CatBoostClassifier on breast cancer dataset\n",
        "!pip install catboost\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "\n",
        "# 1. Load\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# 3. Create CatBoost Pool (no categorical features in this dataset)\n",
        "train_pool = Pool(X_train, y_train)\n",
        "test_pool = Pool(X_test, y_test)\n",
        "\n",
        "# 4. Define model with sensible defaults\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    loss_function='Logloss',\n",
        "    eval_metric='AUC',\n",
        "    random_seed=42,\n",
        "    verbose=100,\n",
        "    early_stopping_rounds=50,\n",
        "    task_type='CPU',  # or 'GPU' if available\n",
        "    use_best_model=True\n",
        ")\n",
        "\n",
        "# 5. Fit\n",
        "model.fit(train_pool, eval_set=test_pool)\n",
        "\n",
        "# 6. Evaluate\n",
        "preds_proba = model.predict_proba(X_test)[:,1]\n",
        "preds = (preds_proba > 0.5).astype(int)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
        "print(\"AUC:\", roc_auc_score(y_test, preds_proba))\n",
        "print(classification_report(y_test, preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYKOZXFW8N5c"
      },
      "source": [
        "# Practical demo 2 — pipeline + hyperparameter search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHAa5BVy8d-v",
        "outputId": "ac654be7-19d7-476e-e583-334d9c8ef6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best score: 0.9733285917496444\n",
            "Best params: {'bagging_temperature': np.float64(0.5618101782710437), 'depth': 7, 'iterations': 800, 'l2_leaf_reg': np.float64(8.31993941811405), 'learning_rate': np.float64(0.12973169683940733)}\n"
          ]
        }
      ],
      "source": [
        "# Demo 2: Pipeline + RandomizedSearchCV using Iris (with a synthetic categorical feature)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 1. Load and create synthetic categorical column\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = iris.target\n",
        "\n",
        "# create a categorical feature by binning petal length into 3 categories\n",
        "X['petal_length_cat'] = pd.cut(X['petal length (cm)'], bins=3, labels=['short', 'medium', 'long'])\n",
        "\n",
        "# our categorical column is last column index\n",
        "cat_feature_name = 'petal_length_cat'\n",
        "cat_features = [X.columns.get_loc(cat_feature_name)]  # pass indices to CatBoost\n",
        "\n",
        "# 2. For CatBoost we can pass raw DataFrame (no need to one-hot). But scikit-learn's RandomizedSearchCV expects an estimator.\n",
        "cbc = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=6,\n",
        "    loss_function='MultiClass',\n",
        "    random_seed=42,\n",
        "    verbose=0,  # silence\n",
        "    early_stopping_rounds=50,\n",
        "    use_best_model=False # Changed this from True to False\n",
        ")\n",
        "\n",
        "# 3. Parameter distributions for RandomizedSearch\n",
        "from scipy.stats import randint, uniform\n",
        "param_dist = {\n",
        "    'learning_rate': uniform(0.01, 0.2),  # 0.01 - 0.21\n",
        "    'depth': randint(3, 9),               # 3 - 8\n",
        "    'l2_leaf_reg': uniform(1, 10),        # 1 - 11\n",
        "    'bagging_temperature': uniform(0, 1.5),\n",
        "    'iterations': [200, 500, 800]\n",
        "}\n",
        "\n",
        "# 4. Create RandomizedSearchCV and fit (note: we pass cat_features at fit time)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=cbc,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    cv=4,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=1  # CatBoost can be multi-threaded internally; set n_jobs=1 to avoid conflicts\n",
        ")\n",
        "\n",
        "# 5. Fit with cat_features passed as fit param (scikit-learn will forward it to CatBoost.fit)\n",
        "search.fit(X, y, cat_features=cat_features)\n",
        "\n",
        "# 6. Best result\n",
        "print(\"Best score:\", search.best_score_)\n",
        "print(\"Best params:\", search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJH6mHHM8-PD"
      },
      "source": [
        "# CatBoost: Practical Implementation & Strategy Guide\n",
        "\n",
        "### Implementation Notes\n",
        "* **Kwargs Forwarding:** Passing `cat_features` into `search.fit(...)` works because `RandomizedSearchCV` forwards extra keyword arguments directly to the underlying estimator’s `.fit()` method.\n",
        "* **No Scaling:** Numerical features are kept in their raw state. Scaling is only necessary if CatBoost is part of a pipeline with distance-based models (KNN, SVM, etc.).\n",
        "* **Hardware:** For massive event datasets, toggle `task_type='GPU'` and specify `devices='0'`.\n",
        "\n",
        "---\n",
        "\n",
        "### The Tuning Checklist\n",
        "| Focus Area | Strategy |\n",
        "| :--- | :--- |\n",
        "| **Learning Rate** | Lower `learning_rate` generalizes better; increase `iterations` to compensate. Use early stopping. |\n",
        "| **Tree Depth** | Small data: `4–6`. Complex data: `7–10`. Higher depth = higher overfitting risk. |\n",
        "| **Regularization** | Use `l2_leaf_reg`, `random_strength`, and `bagging_temperature` to manage variance. |\n",
        "| **Categoricals** | Use `one_hot_max_size` to toggle between OHE and mean encoding. |\n",
        "| **Imbalance** | Set `auto_class_weights='Balanced'` for lopsided datasets (e.g., \"Will they show up?\" vs \"No-shows\"). |\n",
        "| **Validation** | Use **Stratified CV** for classification; use **Time-Series CV** for event data with temporal trends. |\n",
        "\n",
        "---\n",
        "\n",
        "### Bias-Variance Tradeoff (The \"Tuning Knobs\")\n",
        "\n",
        "\n",
        "\n",
        "* **To Fix Bias (Underfitting):** * ↑ `iterations`, ↑ `depth`, ↑ `border_count`.\n",
        "    * ↓ `l2_leaf_reg`.\n",
        "* **To Fix Variance (Overfitting):**\n",
        "    * ↓ `depth`, ↓ `iterations` (Early Stopping).\n",
        "    * ↑ `l2_leaf_reg`, ↑ `bagging_temperature`, ↑ `random_strength`.\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations & Edge Cases\n",
        "* **Sparse Data:** Linear models or Embeddings often outperform CatBoost on high-dimensional sparse data (e.g., large-scale TF-IDF).\n",
        "* **Small Data:** High risk of overfitting; require heavy regularization.\n",
        "* **Temporal Leakage:** Naive CV fails on time-series data; use ordered splits to avoid \"predicting the past using the future.\"\n",
        "* **Calibration:** For critical probability tasks, we may need Isotonic Regression or Platt Scaling post-training.\n",
        "\n",
        "---\n",
        "\n",
        "### Advanced Interview Concepts\n",
        "* **Ordered Boosting:** A permutation-based approach to compute categorical statistics that prevents target leakage.\n",
        "* **Oblivious Trees:** Symmetric trees that use the same split across an entire level, leading to faster inference and better generalization.\n",
        "* **Native Text Support:** Handles `text_features` directly via built-in bag-of-ngrams or embedding extraction.\n",
        "* **Interpretability:** Native SHAP integration via `get_feature_importance(type='ShapValues')`.\n",
        "\n",
        "---\n",
        "\n",
        "### Interview Q&A Quick-Fire\n",
        "**Q: Why CatBoost over XGBoost/LightGBM?**\n",
        "**A:** Superior handling of categorical features, faster inference via symmetric trees, and reduced leakage through ordered boosting.\n",
        "\n",
        "**Q: Does it need scaling?**\n",
        "**A:** No. Decision trees are invariant to monotonic transformations.\n",
        "\n",
        "**Q: What is Ordered Boosting?**\n",
        "**A:** It computes categorical statistics using only \"past\" data points in a random permutation to avoid looking at the label of the current object.\n",
        "\n",
        "---\n",
        "\n",
        "### Pro-Tips & \"Gotchas\"\n",
        "* **Reproducibility:** Always set `random_seed`.\n",
        "* **Consistency:** Ensure `cat_features` are passed identically during CV and final `.fit()`.\n",
        "* **Visualization:** Use `plot=True` in Jupyter to see real-time loss curves.\n",
        "* **Pipelines:** It is often easier to keep CatBoost *outside* a standard Scikit-Learn `ColumnTransformer` if we want it to handle raw categories itself."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
